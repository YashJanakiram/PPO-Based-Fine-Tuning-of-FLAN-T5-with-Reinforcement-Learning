# PPO-Based-Fine-Tuning-of-FLAN-T5-with-Reinforcement-Learning for Generating Less Toxic Content

This project focuses on fine-tuning a FLAN-T5 language model to reduce the toxicity of its generated outputs. The approach uses Meta AI’s hate speech reward model a binary classifier that labels text as either “hate” or “not hate” as a feedback mechanism. During training, Proximal Policy Optimization (PPO) is applied to reinforce non-toxic generations by rewarding outputs that are classified as non-hateful. This method allows the model to adapt its behavior based on toxicity feedback, ultimately improving the safety and quality of its responses.

## Project Overview

We aim to reduce the toxicity of text generated by a language model through reinforcement learning.This is achieved through the following steps:

1. **Dataset Preparation**: Collect and preprocess a dataset containing toxic and non-toxic text samples.
2. **Reward Model Training**: Train a binary hate speech classifier (based on Meta AI's reward model) to predict whether a given text is "hate" or "not hate".
3. **Fine-Tuning FLAN-T5 with PPO**: Use PPO to fine-tune the FLAN-T5 model. The classifier’s output is used as a reward signal guiding the model toward generating non-toxic content.
4. **Evaluation**: Assess the fine-tuned model using metrics such as:Toxicity Score (via the reward model), Generation Quality (fluency, coherence) and BLEU, ROUGE, or other standard text metrics

## Requirements

- Python >= 3.6
- PyTorch
- Transformers
- Meta AI's hate speech reward model
- FLAN-T5 model
- Dataset containing toxic and non-toxic text samples

## Usage

1. Clone the repository and navigate to the project directory.
2. Install the required dependencies listed in the `requirements.txt` file.
3. Prepare the dataset and preprocess the text samples.
4. Train the hate speech reward model using the prepared dataset.
5. Fine-tune the FLAN-T5 model using PPO and the trained reward model.
6. Evaluate the fine-tuned model's performance on the test set.
7. Adjust hyperparameters and experiment with different configurations to improve results if necessary.

## References

- Meta AI Hate Speech Reward Model: [Link](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)
- FLAN-T5 Model: [Link](https://huggingface.co/docs/transformers/model_doc/flan-t5)
- Proximal Policy Optimization (PPO): [Documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html#proximal-policy-optimization)
- Dataset: [Link](https://huggingface.co/datasets/knkarthick/dialogsum)
